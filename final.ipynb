{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e4152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.feature import hog\n",
    "\n",
    "import pickle\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd176e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ------------------ 偵測圖中具有文字的目標位置 ------------------  \"\"\"\n",
    "\n",
    "\"\"\" ## ==================== 呼叫Yolov3 ====================  ## \"\"\"\n",
    "def Load_YoloV3(model_weights,model_cfg):\n",
    "    return cv2.dnn.readNet(model_weights,model_cfg)\n",
    "\n",
    "\"\"\" ## ==================== 畫出BoundingBox 並將之切割出來 ====================  ## \"\"\"\n",
    "def Get_BoundingBox(path_InputImage,model_yolov3,scaling_ratio,threshold = 0.3):\n",
    "    img = cv2.resize(cv2.imread(path_InputImage),None , fx = scaling_ratio , fy = scaling_ratio)\n",
    "    height , width , channels = img.shape\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    model_yolov3.setInput(blob)\n",
    "    layer_names = model_yolov3.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in model_yolov3.getUnconnectedOutLayers()]\n",
    "    outputs = model_yolov3.forward(output_layers)\n",
    "    \n",
    "    boundingBox = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > threshold:\n",
    "                \n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boundingBox.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    indexes = cv2.dnn.NMSBoxes(boundingBox, confidences, 0.5, 0.4)\n",
    "    if indexes == ():\n",
    "        return ()\n",
    "    else:\n",
    "        for i in range(len(boundingBox)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boundingBox[i]\n",
    "        return (x,y,w,h,img)\n",
    "    \n",
    " \n",
    "\n",
    "\"\"\" ## ====================  Image Prepocessing ====================  ## \"\"\"\n",
    "\n",
    "def Thereshold_Image(img_gray,thresh=127,max_=255):\n",
    "    return cv2.threshold(img_gray,thresh,max_,cv2.THRESH_BINARY|cv2.THRESH_OTSU)    \n",
    "    \n",
    "def Get_CharsBoundingBox(image,model_yolov3,scaling_ratio,threshold = 0.3):\n",
    "    img = cv2.resize( image ,None , fx = scaling_ratio , fy = scaling_ratio)\n",
    "    height , width , channels = img.shape\n",
    "    \n",
    "    blob = cv2.dnn.blobFromImage(img, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    model_yolov3.setInput(blob)\n",
    "    layer_names = model_yolov3.getLayerNames()\n",
    "    output_layers = [layer_names[i - 1] for i in model_yolov3.getUnconnectedOutLayers()]\n",
    "    outputs = model_yolov3.forward(output_layers)\n",
    "    \n",
    "    boundingBox = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > threshold:\n",
    "                \n",
    "                # Object detected\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boundingBox.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    indexes = cv2.dnn.NMSBoxes(boundingBox, confidences, 0.5, 0.4)\n",
    "    if indexes == ():\n",
    "        return ()\n",
    "    else:\n",
    "        boxes = []\n",
    "        for i in range(len(boundingBox)):\n",
    "            if i in indexes:\n",
    "                x, y, w, h = boundingBox[i]\n",
    "                x = math.ceil(x/scaling_ratio)\n",
    "                y = math.ceil(y/scaling_ratio)\n",
    "                w = math.ceil(w/scaling_ratio)\n",
    "                h = math.ceil(h/scaling_ratio)\n",
    "                boxes.append([x,y,w,h])\n",
    "        return boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48449294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5bf568",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yolo_cfg = 'your yolov3.cfg path with format .cfg'\n",
    "yolo_weights = 'your yolov3.model_weights path with format .weights'\n",
    "yolo_chars_weights = 'your yolov3.model_weights path (to detect chars) with format .weights'\n",
    "original_image_path = 'your images path'\n",
    "\n",
    "scaling_ratio = 0.4\n",
    "images_path = glob.glob(original_image_path)\n",
    "net = Load_YoloV3(yolo_weights , yolo_cfg)\n",
    "net_chars = Load_YoloV3(yolo_chars_cfg , yolo_weights)\n",
    "\n",
    "\"\"\"\n",
    ": params : image_unable_crop : collect the images name which are unable to be detected out\n",
    ": params : image_crop : collect those results which are detected by yolov3 \n",
    "\n",
    "# Try to reduce confidences scores in NMS(Non Maximum Suppression)\n",
    "## From 0.4 --> 0.3\n",
    "\n",
    "\"\"\"\n",
    "image_unable_crop = [] \n",
    "image_crop = [] \n",
    "for k in tqdm(range(len(images_path))):\n",
    "    image_name = images_path[k]\n",
    "    bounding_box = Get_BoundingBox(images_path[k],net,scaling_ratio = scaling_ratio)\n",
    "    if bounding_box == ():\n",
    "        image_unable_crop.append(image_name)\n",
    "        print(k , image_name)\n",
    "        pass\n",
    "    else:\n",
    "        x , y , w , h , img = bounding_box\n",
    "        x = x - 25 if x-25 > 0 else 0\n",
    "        w = w + 50\n",
    "        y = y - 5 if y - 5 > 0 else 0\n",
    "        h = h + 10\n",
    "        img_crop = img[ y : y+h , x : x+w ]\n",
    "        h,w = img_crop.shape[:2]\n",
    "        h,w = math.ceil(w/scaling_ratio),math.ceil(h/scaling_ratio)\n",
    "        img_crop = cv2.resize(img_crop,(h,w))\n",
    "        image_crop.append([image_name,img_crop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b25a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    ": params : Chars_ : To collect the bouding boxes index of each characters\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Chars_ = []\n",
    "for i in tqdm(range(len(image_crop))):\n",
    "    name_image = image_crop[i][0].split('\\\\')[-1][:-4]\n",
    "    img = cv2.resize(image_crop_[i],(700,150))\n",
    "    chars_boxes = sorted(Get_CharsBoundingBox(img,net_chars,scaling_ratio))\n",
    "    if chars_boxes == ():\n",
    "        image_unable_to_predict.append(name_image)\n",
    "    else:    \n",
    "        chars = []\n",
    "        for k in range(len(chars_boxes)):\n",
    "            x,y,w,h = chars_boxes[k]\n",
    "            initial_x = x\n",
    "            initial_y = y\n",
    "            final_x = x + w\n",
    "            final_y = y + h\n",
    "            chars.append([name_image,initial_y , final_y , initial_x , final_x])\n",
    "        Chars_.append(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b735393",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# To detect the original images are vertical flipped or not\n",
    "\n",
    ": params : Chars_Rotated_ : To collect the chars images after detection\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Chars_Rotated_ = []\n",
    "for m in tqdm(range(len(image_crop))):\n",
    "    img_ = cv2.resize(image_crop[m][1],(700,150))\n",
    "    \n",
    "    if Chars_[m] == []:\n",
    "        Chars_Rotated_.append([])\n",
    "    \n",
    "    else:\n",
    "        if len(Chars_[m]) < 2:\n",
    "            char_rotated_ = []\n",
    "            for n in range(len(Chars_[m])):\n",
    "                initial_y = Chars_[m][n][1][0]\n",
    "                final_y = Chars_[m][n][1][1]\n",
    "                initial_x = Chars_[m][n][1][2]\n",
    "                final_x = Chars_[m][n][1][3]\n",
    "                char_rotated_.append(img_[initial_y:final_y,initial_x:final_x])\n",
    "            Chars_Rotated_.append(char_rotated_)\n",
    "        \n",
    "        else:\n",
    "            index_ = [0]\n",
    "            distance = []\n",
    "            for j in range(len(Chars_[m])-1):\n",
    "                distance.append(Chars_[m][j+1][1][2] - Chars_[m][j][1][3])\n",
    "            index_ = np.append(index_,np.where(np.array(distance) >= round(np.mean(distance))+5)[0])\n",
    "            if len(index_) <= 4:\n",
    "                num_split = [0]*3\n",
    "                num_split = num_split[:len(index_)]\n",
    "                for k in range(len(index_)-1):\n",
    "                    num_split[k] = len(Chars_[m][ index_[k] :index_[k+1] ])\n",
    "                num_split[0] = num_split[0] +1\n",
    "                num_split[-1] = len(Chars_[m]) - np.sum(num_split)\n",
    "            else:\n",
    "                num_split = 0\n",
    "    \n",
    "            if num_split == 0:\n",
    "                char_rotated_ = []\n",
    "                for n in range(len(Chars_[m])):\n",
    "                    initial_y = Chars_[m][n][1][0]\n",
    "                    final_y = Chars_[m][n][1][1]\n",
    "                    initial_x = Chars_[m][n][1][2]\n",
    "                    final_x = Chars_[m][n][1][3]\n",
    "                    char_rotated_.append(img_[initial_y:final_y,initial_x:final_y])\n",
    "                Chars_Rotated_.append(char_rotated_)\n",
    "            else:\n",
    "                if num_split[0] < num_split[-1]:\n",
    "                    char_rotated_ = []\n",
    "                    for n in range(len(Chars_[m])):\n",
    "                        char_rotated_.append(Chars_[m][n][1])\n",
    "                    char_rotated_ = sorted(char_rotated_,key=lambda x:x[2])[::-1]\n",
    "                \n",
    "                    img_rotated = []\n",
    "                    for n in range(len(char_rotated_)):\n",
    "                        initial_y = char_rotated_[n][0]\n",
    "                        final_y = char_rotated_[n][1]\n",
    "                        initial_x = char_rotated_[n][2]\n",
    "                        final_x = char_rotated_[n][3]\n",
    "                        img_rotated.append(cv2.rotate(img_[initial_y:final_y,initial_x:final_x],cv2.ROTATE_180))\n",
    "                    Chars_Rotated_.append(img_rotated)\n",
    "                \n",
    "                else:\n",
    "                    char_rotated_ = []\n",
    "                    for n in range(len(Chars_[m])):\n",
    "                        initial_y = Chars_[m][n][1][0]\n",
    "                        final_y = Chars_[m][n][1][1]\n",
    "                        initial_x = Chars_[m][n][1][2]\n",
    "                        final_x = Chars_[m][n][1][3]\n",
    "                        char_rotated_.append(img_[initial_y:final_y,initial_x:final_x])\n",
    "                    Chars_Rotated_.append(char_rotated_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd314aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Final Checking the splitted chars\n",
    "## If there are images with too large width --> it is possible that contains more than one chars in one images\n",
    " width_of_images > 90 : high probability that contains more chars \n",
    " \n",
    "\"\"\"\n",
    "Chars_Final_Modify = []\n",
    "for i in tqdm(range(len(Chars_Rotated_))):\n",
    "    chars_final_modify = []\n",
    "    for j in range(len(Chars_Rotated_[i])):\n",
    "        if Chars_Rotated_[i] == [] :\n",
    "            chars_final_modify.append(Chars_Rotated_[i])\n",
    "        else:\n",
    "            if Chars_Rotated_[i][j].shape ==0:\n",
    "                pass\n",
    "            else:\n",
    "                _ , w , _ = Chars_Rotated_[i][j].shape\n",
    "                if w <= 90:\n",
    "                    chars_final_modify.append(Chars_Rotated_[i][j])\n",
    "                else:\n",
    "                    center_x = w // 2\n",
    "                    for n in range(2):\n",
    "                        chars_final_modify.append(Chars_Rotated_[i][j][:,center_x*i:center_x*(i+1)])\n",
    "    Chars_Final_Modify.append(chars_final_modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d28dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# HOG features\n",
    "## with params : { 'orientations' : 8 , 'pixels_per_cell' : (9, 9) , 'cells_per_block' : (1, 1) }\n",
    "              \n",
    "\"\"\"\n",
    "\n",
    "HOG = []\n",
    "for i in tqdm(range(len(Chars_Rotated_))):\n",
    "    if len(Chars_Rotated_[i]) == 0:\n",
    "        HOG.append(' ')\n",
    "    else:\n",
    "        hog_features = []\n",
    "        for j in range(len(Chars_Rotated_[i])):\n",
    "            if Chars_Rotated_[i][j].size == 0:\n",
    "                pass\n",
    "            else:\n",
    "                img_gray = cv2.cvtColor(Chars_Rotated_[i][j],cv2.COLOR_BGR2GRAY)\n",
    "                img_resize = cv2.resize(img_gray,(227,227))\n",
    "                _,thresh = cv2.threshold(img_resize,125,255,cv2.THRESH_BINARY|cv2.THRESH_OTSU)\n",
    "                hog_features.append(hog(thresh, orientations = 8 , pixels_per_cell = (9, 9),\n",
    "                                        cells_per_block = (1, 1) , visualize=False))\n",
    "        HOG.append(hog_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fae6d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_xgb = 'path of xgboost model trained'\n",
    "xgb_model = pickle.load(open(path_xgb, 'rb'))\n",
    "\n",
    "mapping = {'0': '0','1': '1','2': '2','3': '3','4': '4','5': '5',\n",
    "           '6': '6','7': '7','8': '8','9': '9','10': 'A','11': 'B',\n",
    "           '12': 'C','13': 'D','14': 'E','15': 'F','16': 'G',\n",
    "           '17': 'H','18': 'J','19': 'K','20': 'L','21': 'M',\n",
    "           '22': 'N','23': 'P','24': 'Q','25': 'R','26': 'S',\n",
    "           '27': 'T','28': 'U','29': 'V','30': 'W','31': 'X',\n",
    "           '32': 'Y','33': 'Z'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9588e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Prediction\n",
    ": params : Pred : Collect the final predicting results\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "Pred = []\n",
    "for i in tqdm(range(len(HOG))):\n",
    "    if HOG[i] == ' ':\n",
    "        Pred .append(' ')\n",
    "    else:\n",
    "        pred_xgb = xgb_model.predict(HOG[i])\n",
    "        Pred.append(''.join([mapping[str(pred_xgb[j])] for j in range(len(pred_xgb))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97092e6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
